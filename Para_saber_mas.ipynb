{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fd70de",
   "metadata": {},
   "source": [
    "# Bloque 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f8e1a",
   "metadata": {},
   "source": [
    "## ¿qué es Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8c296",
   "metadata": {},
   "source": [
    "El **Inteligencia Artificial (IA)** se ha destacado como una de las áreas de mayor crecimiento y visibilidad en los últimos años. Es un campo de estudio amplio que abarca diversas áreas del conocimiento, tanto prácticas como teóricas, incluyendo la ciencia de la computación, la ciencia cognitiva, la filosofía de la mente y el **Machine Learning** (Aprendizaje Automático).\n",
    "\n",
    "El Machine Learning (ML), como una subárea de la inteligencia artificial, se concentra en el desarrollo de **algoritmos** que son utilizados en la computadora para realizar tareas sin la necesidad de programar explícitamente las reglas que se utilizarán. Estos algoritmos basan sus decisiones a partir de datos con el objetivo de comprender e identificar el patrón existente en esos datos, para luego utilizar ese conocimiento en la realización de predicciones.\n",
    "\n",
    "**Cómo funciona el Machine Learning**\n",
    "\n",
    "El funcionamiento del Machine Learning tiene 3 etapas principales:\n",
    "\n",
    "**1 - Recolección de datos**\n",
    "\n",
    "La primera etapa de un proyecto de ML es la extracción o recolección de datos. Los datos son esenciales y pueden considerarse la materia prima de los algoritmos. La cantidad y calidad de estos datos tienen un impacto muy grande en el aprendizaje de los modelos. Con pocos datos, el modelo puede no tener información suficiente para aprender. Con datos de poca calidad, el modelo puede no ser capaz de diferenciar bien el patrón de los datos o comprender el patrón de manera diferente a lo que ocurre con los datos del mundo real.\n",
    "\n",
    "**2 - Entrenamiento de los modelos**\n",
    "\n",
    "Después de recolectar datos y asegurar que tienen calidad, se llega a la etapa de entrenar los modelos. El entrenamiento consiste en que el algoritmo busque el patrón presente en los datos y construya una regla para tomar decisiones posteriormente en nuevos datos.\n",
    "\n",
    "**3 - Evaluación**\n",
    "\n",
    "Con el modelo entrenado, se llega a la etapa de evaluar el desempeño del modelo, para identificar si realmente aprendió el patrón de los datos y si es capaz de aplicar de forma satisfactoria la regla generada por el algoritmo en datos nuevos, que no fueron utilizados durante el momento del entrenamiento.\n",
    "\n",
    "A pesar de ser las etapas principales, estas no son las únicas tareas presentes en proyectos de Machine Learning. Cada proyecto tiene sus propias características, ya sea por el formato y naturaleza de los datos, el tipo de aplicación o los desafíos encontrados a lo largo del proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f97dc",
   "metadata": {},
   "source": [
    "## ¿qué es clasificación?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fd28c",
   "metadata": {},
   "source": [
    "Dentro del área de **Machine Learning**, existen diferentes tipos de aprendizaje: el supervisado, semi supervisado, no supervisado y por refuerzo. La tarea de clasificación es solo una entre las tareas realizadas por los algoritmos, que forma parte del aprendizaje supervisado. Con el organigrama a continuación, observa los flujos que se establecen en estas relaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58130daf",
   "metadata": {},
   "source": [
    "![01](img/01.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7dd67",
   "metadata": {},
   "source": [
    "alt text: organigrama con los tipos de aprendizaje en Machine Learning. El nivel más alto de la jerarquía del organigrama comienza con Tipos de aprendizaje en Machine Learning del cual se desprenden 4 conexiones hacia los subítems: aprendizaje supervisado, aprendizaje semi supervisado, aprendizaje no supervisado y aprendizaje por refuerzo. El subítem aprendizaje supervisado tiene dos conexiones con los ítems: Clasificación, que tiene un destacado en verde oscuro y Regresión\n",
    "\n",
    "Imagina un grupo de jóvenes que está aprendiendo a identificar diferentes estilos musicales. Tienen un mentor que posee una colección de canciones en su dispositivo de audio, cada una debidamente etiquetada con el género musical correspondiente, pero los jóvenes no pueden distinguir los géneros musicales por sí mismos. Al principio, el mentor comienza a reproducir varias canciones y, al mismo tiempo, dice cuál es el género de cada una de ellas. Ellos escuchan con atención y, a lo largo del tiempo, comienzan a asociar las características musicales, como el ritmo, la instrumentación y los vocales, a los diferentes géneros.\n",
    "\n",
    "En este ejemplo, los jóvenes se basan en algunas características como ritmos rápidos y vocales enérgicos que encajan en el género pop, mientras que las canciones con guitarras distorsionadas y vocales más intensos se dirigen al género rock. Con base en este proceso, los jóvenes logran identificar el género de nuevas canciones que no fueron previamente etiquetadas por el mentor, pero usando las reglas que aprendieron, fueron capaces de etiquetar y nombrar los nuevos elementos.\n",
    "\n",
    "El aprendizaje supervisado en Machine Learning sigue este mismo razonamiento. Utiliza conjuntos de datos etiquetados, es decir, bases de datos con registros históricos que contienen la respuesta correcta en cada uno de los registros. Para entonces, a partir de esta respuesta y de las características de los datos, el algoritmo puede trazar una regla para llegar a la respuesta que podrá ser utilizada posteriormente en nuevos datos, con el fin de hacer una predicción.\n",
    "\n",
    "La característica principal de la clasificación se da por el tipo de dato presente en la respuesta, que debe ser del tipo categórica. Un dato del tipo categórico es aquel que tiene diferentes clases o categorías. Como ejemplos de aplicaciones de clasificación con Machine Learning, tenemos:\n",
    "\n",
    "- Filtrado de correos electrónicos spam\n",
    "- Diagnósticos médicos\n",
    "- Análisis textual de sentimientos\n",
    "- Detección de fraudes bancarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028caf1a",
   "metadata": {},
   "source": [
    "## tipos de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b089465",
   "metadata": {},
   "source": [
    "En una base de datos utilizada en proyectos de Machine Learning, podemos llamar a las columnas variables. Este concepto, proveniente de la estadística, representa una característica de interés que se mide en cada elemento de una muestra o población. El nombre indica que el valor varía de elemento a elemento, pudiendo tener valores numéricos o no numéricos.\n",
    "\n",
    "Las variables se dividen de la siguiente forma:\n",
    "\n",
    "**Variables cuantitativas o numéricas**\n",
    "\n",
    "Son características que pueden ser medidas a partir de valores numéricos que tienen sentido y se dividen entre variables discretas y continuas.\n",
    "\n",
    "- **Variables discretas**: características medidas solo por un número finito o contable de valores. Solo tiene sentido para valores enteros. Por ejemplo: número de hijos, número de ventas.\n",
    "- **Variables continuas**: características medidas que asumen valores en una escala continua (en la recta real), en la que tienen sentido valores fraccionarios. Ejemplo: peso, tiempo, altura.\n",
    "\n",
    "**Variables cualitativas o categóricas**\n",
    "\n",
    "Son características que no poseen valores cuantitativos y se definen por varias categorías o clases. Se dividen en nominales y ordinales.\n",
    "\n",
    "- **Variables nominales**: no existe ordenación entre las categorías. Ejemplo: sexo biológico, país, churn.\n",
    "- **Variables ordinales**: existe una ordenación entre las categorías. Ejemplo: escolaridad, mes.\n",
    "\n",
    "```Atención: Un punto importante a tratar es que no siempre una variable representada por números es cuantitativa.```\n",
    "\n",
    "Por eso, es esencial tener una postura crítica y evaluar la información detrás del dato, y no solo fijarse en la forma en que está disponible. Por ejemplo, una información de ID de registro puede ser un número, sin embargo, su función está en categorizar un elemento. De la misma forma, una información de escolaridad puede estar representada con los valores 1, 2 y 3, y aun así no los transforma en una variable numérica. Esta información no puede ser utilizada para hacer sumas y calcular promedios, por ejemplo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440906c",
   "metadata": {},
   "source": [
    "# Bloque 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19f66a",
   "metadata": {},
   "source": [
    "## biblioteca Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d8070",
   "metadata": {},
   "source": [
    "Además de los datos, hay algo que se vuelve indispensable en los proyectos de Machine Learning, que son los **algoritmos**. Es claro que no necesitamos crear los algoritmos desde cero, están disponibles de forma gratuita a través de una biblioteca del lenguaje Python, **Scikit-Learn**. Esta ofrece no solo una amplia variedad de algoritmos, sino también herramientas de preprocesamiento de datos, análisis y evaluación de modelos.\n",
    "\n",
    "Uno de los puntos más positivos de la biblioteca es su [documentación](https://scikit-learn.org/stable/index.html), que está bien organizada y tiene una navegación intuitiva. Contiene la explicación y ejemplos de uso de todas las funciones, además de información teórica sobre diversos temas relacionados con Machine Learning. La documentación sin duda debe formar parte del día a día de la persona científica de datos, desde el nivel inicial hasta el más avanzado de conocimiento.\n",
    "\n",
    "El otro punto ventajoso de esta biblioteca es su uso simple. Con pocas líneas de código es posible entrenar un modelo, abstraiendo todos los detalles complejos que ocurren detrás de escena. Por esta razón, se ha convertido en una de las principales bibliotecas para trabajar con datos y, sobre todo, Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f4fcc",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b869f5",
   "metadata": {},
   "source": [
    "Los algoritmos de Machine Learning no pueden comprender información que no esté en formato numérico. Por lo tanto, si se desea utilizar variables categóricas en modelos, es necesario que pasen por algún tipo de tratamiento para que estén en formato numérico. Esto no significa que se convertirán en variables numéricas, solo que estarán en un formato que sea comprendido por los modelos.\n",
    "\n",
    "Así, estas transformaciones deben preservar la información real de las categorías de la mejor manera posible, sin introducir sesgos en el modelo y sin información que esté alejada de la realidad.\n",
    "\n",
    "La forma ideal de hacer este tipo de transformación, que mantiene la información original, se conoce como one hot encoding. Esta acción transforma cada una de las clases de las variables categóricas en nuevas columnas, utilizando el valor 0 para representar la ausencia de la característica y 1 para la presencia de la característica en la muestra de la base de datos. Observa en detalle el dinamismo de este proceso en la imagen a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2b60d",
   "metadata": {},
   "source": [
    "![02](img/02.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a209b1",
   "metadata": {},
   "source": [
    "Hay una forma muy simple de hacer esta transformación usando la biblioteca pandas, a partir de la función ``pd.get_dummies()``, sin embargo, no es un método muy recomendado cuando estamos trabajando con Machine Learning, ya que esta función no puede abstraer y ejecutar la misma transformación para un nuevo dato. Si tienes una nueva información que pertenece solo a una de las clases de una variable objetivo, el proceso de ``get_dummies`` no será capaz de generar las otras columnas provenientes de las otras clases. Esto se convierte en un problema para el modelo, ya que espera todas las características para realizar una predicción.\n",
    "\n",
    "El método más recomendado para realizar la transformación en proyectos de Machine Learning es el ``OneHotEncoder``. En un primer momento, con los datos iniciales, comienza su acción comprendiendo las características de los datos y genera las nuevas columnas para cada clase. Además, almacena la regla capaz de hacer este procedimiento para nuevos datos. Por lo tanto, en el proceso de transformación de un nuevo dato, puede crear todas las columnas necesarias, aunque este nuevo dato tenga solo la información de una de las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59939b79",
   "metadata": {},
   "source": [
    "# Bloque 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80473a",
   "metadata": {},
   "source": [
    "## Overfitting y Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e49ed04",
   "metadata": {},
   "source": [
    "Una tarea muy importante para la evaluación de modelos de machine learning es la división de los datos entre entrenamiento y prueba. El conjunto de entrenamiento se utiliza para que el modelo comprenda patrones y relaciones en los datos para que pueda crear una regla para hacer predicciones. El conjunto de prueba, por su parte, se reserva para evaluar el desempeño del modelo en datos que no se utilizaron en el entrenamiento, simulando la capacidad del modelo de generalizar a nuevos datos.\n",
    "\n",
    "Existen dos conceptos que están muy ligados a esta división de los datos y que son muy relevantes en los proyectos de machine learning: el overfitting y el underfitting.\n",
    "\n",
    "**Overfitting (Sobreajuste):**\n",
    "\n",
    "El overfitting ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento. Esto indica que el modelo capturó no solo el patrón de los datos, sino también ruidos y variaciones aleatorias que están presentes en los datos utilizados para el entrenamiento. Como resultado de esto, el modelo tiene un resultado muy bueno al ser evaluado con los datos de entrenamiento, sin embargo, su desempeño en los datos de prueba o en datos nuevos cae considerablemente.\n",
    "\n",
    "Características del overfitting:\n",
    "\n",
    "- Error muy bajo en las predicciones en datos de entrenamiento;\n",
    "\n",
    "- Error muy alto en las predicciones en datos de prueba;\n",
    "\n",
    "- Modelo muy complejo que intenta memorizar los datos de entrenamiento en lugar de aprender el patrón de los datos.\n",
    "\n",
    "**Underfitting (Subajuste):**\n",
    "\n",
    "El underfitting ocurre cuando un modelo es muy simple y no puede capturar el patrón presente en los datos. Esto indica que el modelo no fue capaz de aprender las relaciones existentes en los datos de entrenamiento y termina teniendo un desempeño deficiente tanto en datos de entrenamiento como de prueba.\n",
    "\n",
    "Características del underfitting:\n",
    "\n",
    "- Error muy alto en las predicciones en datos de entrenamiento;\n",
    "\n",
    "- Error muy alto en las predicciones en datos de prueba;\n",
    "\n",
    "- Modelo muy simple que no puede representar bien los datos.\n",
    "\n",
    "El objetivo principal de la creación de modelos de machine learning es encontrar un equilibrio entre el overfitting y el underfitting, para que haya un ajuste adecuado. Un modelo bien ajustado es capaz de aprender el patrón de los datos y generalizar a nuevos datos, haciendo predicciones con consistencia sin ser demasiado influenciado por los ruidos presentes en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce361f",
   "metadata": {},
   "source": [
    "## cómo funciona el árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676439f",
   "metadata": {},
   "source": [
    "El árbol de decisión es un algoritmo de machine learning supervisado que tiene una buena interpretabilidad. Esto significa que es posible tener una comprensión fácil de los pasos que se realizaron para llegar al resultado final de la predicción del modelo. Estos pasos pueden ser representados de forma visual, a partir de un diagrama que indica cada una de las decisiones que se tomaron para llegar a la clasificación de un dato.\n",
    "\n",
    "Para llegar a una regla que clasifique los datos con una buena tasa de acierto, las decisiones del árbol no pueden ser totalmente aleatorias. Debe haber un sentido en cada elección hecha por el árbol de decisión. Ahora entendamos cómo se hacen estas elecciones:\n",
    "\n",
    "El primer paso es seleccionar una columna de la base de datos que se utilizará para dividir los datos en 2 subconjuntos. El objetivo es que la mayor cantidad posible de datos se separe en relación con la variable objetivo. Entonces, el mejor resultado posible sería si uno de los subconjuntos tuviera solo datos de una categoría de la variable objetivo y el otro subconjunto tuviera solo datos de la otra categoría restante. Para hacer la mejor elección posible, se prueban diferentes columnas y valores, y aquella que proporcione la mejor separación se elige como la primera regla del árbol de decisión.\n",
    "\n",
    "Para definir qué es una buena separación, se realizan cálculos matemáticos para obtener la proporción de datos de cada categoría de la variable objetivo dentro de los subconjuntos. El resultado de este cálculo se conoce como **métrica de impureza**. Existen diferentes tipos de métricas, siendo las más utilizadas la **entropía** y el **índice de Gini**. A continuación, se presentan las características de cada una.\n",
    "\n",
    "**Índice Gini**\n",
    "\n",
    "Este índice informa el grado de heterogeneidad de los datos. Su objetivo es medir la frecuencia de que un elemento aleatorio de un nodo sea etiquetado de manera incorrecta. En otras palabras, este índice cuantifica y determina la impureza de un nodo mediante el siguiente cálculo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0977c5",
   "metadata": {},
   "source": [
    "![03](img/03.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da0fb0",
   "metadata": {},
   "source": [
    "Donde:\n",
    "\n",
    "- ``P(i)`` representa la frecuencia relativa de las clases en cada uno de los nodos;\n",
    "- ``k`` es el número de clases.\n",
    "\n",
    "Si el índice Gini es igual a 0, esto indica que el nodo es puro. Sin embargo, si su valor se aproxima más al valor 1, el nodo es impuro.\n",
    "\n",
    "**Entropía**\n",
    "\n",
    "La idea básica de la entropía es medir **el desorden de los datos** de un nodo mediante la variable clasificadora. Así como el índice de Gini, se utiliza para caracterizar la impureza de los datos y puede ser calculada mediante la siguiente fórmula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18125bc",
   "metadata": {},
   "source": [
    "![04](img/04.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0677fb76",
   "metadata": {},
   "source": [
    "Donde:\n",
    "\n",
    "- ``pi`` representa la proporción de datos en el conjunto de datos, pertenecientes a la clase específica ``i``;\n",
    "\n",
    "- ``c`` es el número de clases.\n",
    "\n",
    "Después de realizar la primera elección de división, el proceso se repite para cada subconjunto hasta que se alcance una condición de parada o que todos los subconjuntos finales estén totalmente puros, es decir, con solo datos de una de las clases de la variable objetivo. A partir de la regla generada, nuevos datos pueden ser clasificados pasando por cada una de las decisiones del árbol hasta llegar a la elección final.\n",
    "\n",
    "Si deseas saber más sobre el árbol de decisión, puedes consultar la documentación de la biblioteca Scikit-Learn, que tiene una explicación detallada de cómo funciona y también de la función implementada con el algoritmo:\n",
    "\n",
    "- [Comprendiendo la estructura del árbol de decisión](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)\n",
    "\n",
    "- [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1483a180",
   "metadata": {},
   "source": [
    "# Bloque 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447273d",
   "metadata": {},
   "source": [
    "## cómo funciona el KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3a813",
   "metadata": {},
   "source": [
    "El modelo k-Nearest Neighbors (KNN) es un algoritmo de machine learning ampliamente utilizado. Es una técnica simple, pero eficaz, que se basa en la idea de que los objetos similares tienden a estar cerca unos de otros en un espacio de características. A continuación, exploraremos cómo funciona el KNN y cómo toma decisiones de clasificación.\n",
    "\n",
    "**Funcionamiento**\n",
    "\n",
    "El algoritmo KNN opera calculando la distancia entre todos los elementos de la base de datos para determinar la clasificación de un registro, lo cual se realiza al verificar las clases de los elementos que están más cerca.\n",
    "\n",
    "En esta dinámica, el primer paso consiste en definir un valor de 'k', que es la cantidad de vecinos más cercanos a considerar al momento de hacer la clasificación. La elección de este valor es importante y afecta el rendimiento del modelo. A continuación, se calcula la distancia entre todos los elementos y se almacenan los resultados de estas distancias.\n",
    "\n",
    "Finalmente, para clasificar cada elemento, se seleccionan los 'k' elementos más cercanos a él y se realiza una votación. La votación consiste en seleccionar la clase que aparece con más frecuencia entre estos vecinos más cercanos.\n",
    "\n",
    "La normalización de los datos es esencial para este algoritmo, porque se basa en cálculos de distancia. Además, es un algoritmo que demanda mucho computacionalmente cuando hay muchos datos, ya que necesita calcular las distancias entre todos los elementos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492b7d0",
   "metadata": {},
   "source": [
    "## Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128e96c",
   "metadata": {},
   "source": [
    "Después de crear los modelos de machine learning, comparar los resultados y seleccionar aquel que obtuvo el mejor rendimiento, es el momento de utilizar el modelo para clasificar nuevos datos del mundo real, que es desde el principio el objetivo real del proyecto. Sucede que el modelo generalmente se construye en un notebook y el entorno en el que se va a utilizar el modelo es diferente, en alguna aplicación, sitio o sistema.\n",
    "\n",
    "Para poder utilizar el modelo, es necesario exportarlo, y es en ese momento que entra en escena el ``pickle``. El módulo ``pickle`` en Python es una herramienta poderosa y versátil que permite la serialización y deserialización de objetos Python. Este proceso de serialización implica la conversión de objetos Python en una representación binaria que puede ser almacenada en un archivo. Más tarde, esta representación puede ser deserializada para recrear el objeto original.\n",
    "\n",
    "Así, es posible almacenar modelos de machine learning en archivos pickle, para que puedan ser utilizados en otros programas. Él preserva completamente el estado del objeto, incluyendo todos los parámetros y configuraciones. Además, el formato binario generado por el ``pickle`` es independiente de la plataforma, lo que significa que es posible crear un archivo en un sistema operativo y cargarlo en otro sin problema de compatibilidad. Vale destacar que en versiones diferentes de Python esto puede ser un problema. Objetos serializados en una versión específica pueden no ser cargados correctamente en otra versión. Por lo tanto, es muy importante saber cuál es la versión del lenguaje y de las bibliotecas utilizadas en el proyecto para que sean replicadas dentro del sistema en el que se va a utilizar.\n",
    "\n",
    "El proceso para utilizar el ``pickle`` involucra principalmente dos funciones:\n",
    "\n",
    "- ``pickle.dump(objeto, archivo)``: Esta función permite almacenar un objeto Python en un archivo. El argumento objeto es el ``objeto`` que deseas serializar, y el argumento ``archivo`` es el objeto de archivo donde deseas almacenar la representación binaria.\n",
    "- ``pickle.load(archivo)``: Esta función permite que deserialices (cargues) un objeto Python de un archivo. El argumento ``archivo`` es el archivo de donde deseas cargar la representación binaria.\n",
    "\n",
    "También podemos usar la biblioteca ``pandas`` para hacer la lectura de archivos ``pickle``. Para esto, basta con utilizar el método ``pd.read_pickle``."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
