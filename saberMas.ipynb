{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de80dbc",
   "metadata": {},
   "source": [
    "# Bloque 1 - Clasificando datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce19f7",
   "metadata": {},
   "source": [
    "## Utilidad del metodo score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45743072",
   "metadata": {},
   "source": [
    "Durante la construcción de un modelo para clasificar transacciones bancarias como legítimas o fraudulentas, una persona científica de datos utilizó la biblioteca Scikit-Learn, que ofrece diversos algoritmos de machine learning que pueden ser utilizados para la clasificación de datos. En cada uno de estos algoritmos, el método ``fit()`` se utiliza para entrenar un modelo a partir de las variables explicativas y la variable respuesta.\n",
    "\n",
    "Sin embargo, después del entrenamiento del modelo, se realiza un paso más mediante el uso del método ``score()``. Seleccione la alternativa que describe de manera adecuada la finalidad de usar el ``score()`` en este contexto:\n",
    "\n",
    "**Se utiliza para evaluar el desempeño del modelo ajustado.**\n",
    "\n",
    "El método ``score()`` tiene la finalidad de comparar las predicciones realizadas por el modelo con los valores reales de los datos y calcula una tasa de acierto en porcentaje llamada exactitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09f397",
   "metadata": {},
   "source": [
    "## parametros test_size=0.15,stratify=y,random_state=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836a88b",
   "metadata": {},
   "source": [
    "¡Hola! Luri aquí. ¡Claro que sí! Con gusto te explico la función de cada uno de esos parámetros en el contexto de la división de datos para entrenamiento y prueba:\n",
    "1. ``test_size=0.15:``\n",
    "Este parámetro define la proporción del conjunto de datos que se utilizará para la prueba (o test). En este caso, ``0.15`` significa que el 15% de los datos originales se reservarán para evaluar el rendimiento final del modelo. El 85% restante se utilizará para entrenamiento y validación.\n",
    "Analogía: Imagina que tienes un pastel y quieres probar si está bien hecho. ``test_size`` sería el tamaño de la porción que cortas para probar (15% del pastel en este caso).\n",
    "\n",
    "\n",
    "2. ``stratify=y:``\n",
    "Este parámetro es crucial cuando tienes un problema de clasificación con clases desbalanceadas (es decir, cuando una clase tiene muchas más muestras que otra). Al usar ``stratify=y``, le indicas a la función que mantenga la proporción de las clases en los conjuntos de entrenamiento y prueba.\n",
    "\n",
    "**¿Por qué es importante?** Si no estratificas, podrías terminar con un conjunto de prueba que no represente bien la distribución original de las clases, lo que llevaría a una evaluación sesgada del modelo.\n",
    "\n",
    "**Analogía**: Imagina que tienes una bolsa con 90 canicas rojas y 10 azules (desbalanceo). Si divides la bolsa al azar, podrías obtener una porción con muchas más canicas rojas que azules, lo cual no sería representativo. ``stratify`` asegura que cada porción tenga aproximadamente la misma proporción de rojas y azules que la bolsa original.\n",
    "\n",
    "**En el contexto de la clase**: En el contexto de la clase, se busca mantener la proporción de clientes morosos en los conjuntos de entrenamiento y prueba.\n",
    "\n",
    "\n",
    "3. ``random_state=5:``\n",
    "Este parámetro controla la aleatoriedad en la división de los datos. Al asignarle un valor entero (en este caso, ``5``), garantizas que la división se realice de la misma manera cada vez que ejecutes el código.\n",
    "\n",
    "**¿Por qué es útil?** Facilita la reproducibilidad de los resultados. Si no fijas un ``random_state``, cada ejecución dividirá los datos de forma diferente, lo que puede llevar a variaciones en el rendimiento del modelo.\n",
    "\n",
    "**Analogía**: Imagina que tienes una baraja de cartas y quieres repartirla al azar. ``random_state`` sería como fijar una semilla para el generador de números aleatorios que se usa para mezclar las cartas. Si usas la misma semilla cada vez, obtendrás la misma secuencia de cartas.\n",
    "\n",
    "\n",
    "\n",
    "En resumen:\n",
    "\n",
    "``test_size`` define cuánto de tus datos usarás para probar tu modelo al final.\n",
    "\n",
    "``stratify`` asegura que las proporciones de las diferentes categorías en tus datos se mantengan al dividir los datos.\n",
    "\n",
    "``random_state`` te permite obtener los mismos resultados cada vez que corres el código, lo cual es útil para la reproducibilidad.\n",
    "\n",
    "Espero que esta explicación te sea de gran ayuda. ¡No dudes en preguntar si tienes más dudas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f414e21",
   "metadata": {},
   "source": [
    "## método holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269fb26",
   "metadata": {},
   "source": [
    "En el momento de validar modelos de clasificación, necesitamos verificar si el modelo está realmente generalizando, es decir, si está comprendiendo el patrón de los datos y clasificando correctamente datos nuevos. La estrategia más simple para evaluar esta generalización, conocida como **holdout**, consiste en dividir los datos en dos partes: un conjunto de datos de **entrenamiento** y otro de **prueba**. El conjunto de entrenamiento se utiliza para entrenar el modelo, mientras que el conjunto de prueba se usa para evaluar el rendimiento del modelo en datos no vistos anteriormente.\n",
    "\n",
    "Con la ayuda de la imagen a continuación, analiza la ejemplificación del método holdout en funcionamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4625489",
   "metadata": {},
   "source": [
    "![01](img/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e114a",
   "metadata": {},
   "source": [
    "En algunos casos, especialmente cuando se realizan ajustes finos en los parámetros del modelo, es útil tener un conjunto de validación adicional. En este caso, la división se hace en tres partes: conjunto de entrenamiento, conjunto de validación y conjunto de prueba. El conjunto de validación se utiliza en la comparación de diferentes modelos, en la selección del modelo más adecuado y en el ajuste de los hiperparámetros. Mientras tanto, el conjunto de prueba sigue utilizándose para evaluar el rendimiento final del modelo elegido, después de todo el proceso de ajuste.\n",
    "\n",
    "Por eso, cuanto más se utilizan los mismos datos para tomar decisiones sobre configuraciones de mejoras en el modelo o elección de hiperparámetros, más comprometida se vuelve la confiabilidad de esos resultados al ser generalizados para datos nuevos y no vistos. Esto ocurre porque las mejoras se hacen a partir de esos datos de validación.\n",
    "\n",
    "Es posible percibir que las mejoras aplicadas desempeñan un papel fundamental para resolver el problema. Sin embargo, para asegurar que el rendimiento del modelo permanezca consistente en relación a los datos del mundo real, que no fueron vistos en el entrenamiento o en la mejora de los modelos, la estrategia de la división entre 3 conjuntos de datos, como se puede analizar en la imagen siguiente, ofrece una buena dirección final, ya que indica si el modelo elegido está sesgado o no en relación a los datos de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f66e4",
   "metadata": {},
   "source": [
    "![02](img/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af290b",
   "metadata": {},
   "source": [
    "## entendiendo la matriz de confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430b0b7",
   "metadata": {},
   "source": [
    "Para obtener una evaluación más completa del desempeño de modelos de clasificación, podemos utilizar una herramienta conocida como matriz de confusión. Esta matriz ofrece ventajas a la persona científica de datos, ya que permite entender cuántos errores y aciertos tiene las predicciones de un modelo. En lugar de una tasa de acierto general, la matriz es capaz de proporcionar información en una visualización para cada una de las categorías de la variable objetivo.\n",
    "\n",
    "Piensa en un sistema de seguridad de un edificio que utiliza cámaras para identificar personas que entran. La \"matriz de confusión\" se vuelve valiosa, ya que permite verificar cuántas veces el sistema acertó al identificar correctamente a las personas autorizadas, cuántas veces acusó erróneamente a personas y cuántas veces dejó pasar a personas no autorizadas. Con estos números, es posible ajustar el sistema para minimizar falsos positivos y negativos, mejorando su precisión en la detección de visitantes.\n",
    "\n",
    "En la representación general de una matriz de confusión, para más detalles analiza la imagen a continuación, las filas de la matriz corresponden a los valores reales de la base de datos, mientras que las columnas corresponden a los valores previstos por el modelo de clasificación. Las categorías de la variable objetivo están representadas por el valor 0 (ausencia del atributo), también llamado negativo, y por el valor 1 (presencia del atributo), también llamado positivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c09fd",
   "metadata": {},
   "source": [
    "![03](img/03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2f1ab",
   "metadata": {},
   "source": [
    "Cada elemento de la matriz está identificado por un nombre de acuerdo con la intersección entre la predicción y el valor real. La diagonal principal de la matriz, que está destacada por el color verde, representa los elementos que tienen la predicción igual al valor real, por lo tanto son los aciertos del modelo. Por otro lado, la diagonal secundaria, que está destacada por el color rojo, representa los elementos con predicciones diferentes del valor real, por lo tanto son los errores del modelo. La descripción de cada uno de los elementos es la siguiente:\n",
    "\n",
    "- **Verdaderos Negativos (VN)**: Cuando el valor real es 0 y la predicción también es 0. Indica que el modelo clasificó correctamente los valores de la clase negativa.\n",
    "\n",
    "- **Falsos Positivos (FP)**: Cuando el valor real es 0 y la predicción es 1. Indica que el modelo clasificó erróneamente un elemento de la clase negativa como si fuera de la clase positiva.\n",
    "\n",
    "- **Falsos Negativos (FN)**: Cuando el valor real es 1 y la predicción es 0. Indica que el modelo clasificó erróneamente un elemento de la clase positiva como si fuera de la clase negativa.\n",
    "\n",
    "- **Verdaderos Positivos (VP)**: Cuando el valor real es 1 y la predicción también es 1. Indica que el modelo clasificó correctamente los valores de la clase positiva.\n",
    "\n",
    "Estos valores son muy útiles para un análisis más profundo del modelo de clasificación. Esto permite identificar las capacidades y limitaciones de la predicción, si hay un equilibrio entre los aciertos y errores o si el resultado está sesgado hacia una clase en detrimento de la otra. Con esto, es evidente que la matriz de confusión es una herramienta mucho más completa que la métrica de precisión, que representa solo el porcentaje de aciertos del modelo, sin considerar las clases de manera aislada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce5e36",
   "metadata": {},
   "source": [
    "# Bloque 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc62e6e",
   "metadata": {},
   "source": [
    "## otros métodos de validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d17f92",
   "metadata": {},
   "source": [
    "Además de la validación cruzada con [``KFold``](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) tradicional y estratificada, existen otros tipos de validación que pueden ser utilizados en proyectos de machine learning. La elección de su uso dependerá de las características de los datos del proyecto. Vamos a explorar más adelante tres nuevos enfoques de separación de los datos utilizados para simular el proceso de aprendizaje en datos futuros.\n",
    "\n",
    "**GroupKFold**\n",
    "\n",
    "El método [``GroupKFold``](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html) es una variación de la validación cruzada KFold tradicional y se utiliza cuando los datos tienen alguna estructura de grupo o dependencia que no debe ser rota, generalmente una característica en una de las columnas de la base de datos.\n",
    "\n",
    "Este enfoque utiliza una estrategia de separación de los datos para que los registros pertenecientes a un grupo específico se mantengan juntos durante las divisiones del KFold, garantizando que no sean separados entre los conjuntos de entrenamiento y validación. Esto es útil para evitar posibles sesgos y garantizar que el modelo generalice para grupos desconocidos, es decir, incluso si no hay datos del grupo en el conjunto de entrenamiento, el modelo deberá tener un buen desempeño al predecir el resultado para los datos de ese grupo.\n",
    "\n",
    "**Leave-p-out**\n",
    "\n",
    "El método [``Leave-p-out``](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html) funciona de manera diferente al método KFold. En lugar de dividir el conjunto de datos en una cantidad fija de conjuntos, se elegirá una cantidad 'p' de elementos para ser dejados fuera del entrenamiento. Los datos se entrenarán en el resto y se validarán solo en los 'p' elementos. Este proceso se repite hasta que todos los datos se utilicen como datos de validación. El resultado final puede considerarse el promedio de los resultados obtenidos en los modelos, tal como se hace en la validación cruzada tradicional.\n",
    "\n",
    "Esto proporciona una validación mucho más completa, ya que considera todas las combinaciones posibles de datos de entrenamiento y validación. Sin embargo, es mucho más costosa computacionalmente, ya que se crearán muchos modelos y esto aumenta a medida que el conjunto de datos es muy grande y el valor elegido para 'p' es pequeño.\n",
    "\n",
    "Leave-one-out\n",
    "El método Leave-one-out es una forma especial del Leave-p-out, donde se elige el valor de p=1. De esta manera, solo se reserva una muestra para validación y todos los demás datos se eligen para entrenamiento. Este proceso se repite para todas las muestras de la base de datos. Esto significa que, si hay 1000 filas en la base de datos, se entrenarán 1000 modelos distintos.\n",
    "\n",
    "Se espera que este método demande mucho computacionalmente, debido a la creación de un modelo para cada fila de la base de datos. Por lo tanto, se indica solo en los casos en que la base de datos es muy pequeña. En estas situaciones, es interesante utilizar la mayor cantidad de datos posible para el entrenamiento, para que el modelo pueda entender el patrón de los datos. Otra estrategia de validación cruzada eliminaría muchos datos que serían útiles en el entrenamiento.\n",
    "\n",
    "Si deseas saber más sobre otros métodos de validación disponibles en la biblioteca Scikit-Learn, puedes consultar la documentación [Validación cruzada: evaluando el desempeño del estimador](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "Si quieres crear una visualización para tener un mejor entendimiento de cómo se realizó la división de los datos en algún proyecto, ya sea con KFold, StratifiedKFold o GroupKFold, puedes explorar la documentación [Visualizando el comportamiento de validación cruzada en scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eeb6a9",
   "metadata": {},
   "source": [
    "# Bloque 4 - Balanceo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e5db2c",
   "metadata": {},
   "source": [
    "En problemas de clasificación, podemos encontrarnos con bases de datos en las que la variable objetivo contenga clases muy desbalanceadas, es decir, categorías con frecuencias muy diferentes. Al entrenar un modelo con la variable desbalanceada, puede ser que el patrón de los datos para la clase dominante sobresalga en relación a la clase con menor frecuencia, generando un modelo con un desempeño muy bajo para clasificar la clase de menor frecuencia.\n",
    "\n",
    "Para sortear estos problemas generados por la base de datos desbalanceada, podemos recurrir a dos soluciones que consisten en equilibrar los datos de la variable objetivo: **undersampling** y **oversampling**. Estas estrategias son útiles para que el modelo pueda comprender mejor el patrón de los datos, pero es importante destacar que también tienen desventajas y consideraciones que debemos analizar antes de utilizarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddb33b",
   "metadata": {},
   "source": [
    "## **Oversampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37ccca",
   "metadata": {},
   "source": [
    "La estrategia de oversampling consiste en aumentar la cantidad de datos de la clase que tiene menor frecuencia hasta que tenga la misma cantidad que la clase de mayor frecuencia. De esta manera, el modelo prestará más atención al patrón de los datos de la clase que tenía menor frecuencia al principio y podrá diferenciar mejor las dos clases.\n",
    "\n",
    "Para aumentar la cantidad de datos necesitamos generar nuevos registros en la base de datos. Es posible utilizar un oversampling aleatorio para duplicar registros de manera aleatoria o usar una técnica como SMOTE para generar datos sintéticos con un patrón cercano a los datos existentes. La desventaja de esta estrategia es la posibilidad de sobreajuste del modelo, especialmente al utilizar el oversampling aleatorio. En este caso, el modelo puede especializarse demasiado en el patrón de los datos que son muy parecidos o idénticos, ya que son copiados o generados sintéticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efdc23f",
   "metadata": {},
   "source": [
    "## **Undersampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16640b",
   "metadata": {},
   "source": [
    "Undersampling\n",
    "La estrategia de undersampling es contraria al oversampling y consiste en reducir la cantidad de datos de la clase que tiene mayor frecuencia hasta que tenga la misma cantidad que la clase de menor frecuencia. De esta forma, el modelo no prestará atención solo a los datos de mayor cantidad y podrá diferenciar mejor las dos clases.\n",
    "\n",
    "Para reducir la cantidad de datos, necesitamos eliminar o borrar registros existentes. Es posible utilizar un undersampling aleatorio para seleccionar los registros que se mantendrán o usar técnicas que seleccionan o eliminan datos a partir de un patrón establecido. La principal desventaja de la estrategia de undersampling es la eliminación de datos que pueden ser muy importantes para la comprensión del problema, especialmente cuando esta eliminación se realiza sin ningún criterio, como es el caso del undersampling aleatorio.\n",
    "\n",
    "Ambas estrategias son válidas para intentar mejorar el desempeño de un modelo de clasificación, pero debemos estar atentos al utilizarlas debido a los puntos negativos que son inherentes a cada uno de los métodos. En cualquier proyecto que utilice alguna de estas herramientas, se debe realizar un análisis para identificar si realmente ayudaron o perjudicaron el desempeño del modelo de clasificación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
